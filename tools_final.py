import os
import re
import time
import json
import weave
import random
import logging
from pathlib import Path
from litellm import completion
from litellm import RateLimitError

# Setup logging
logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

# Litellm needs region to be set
os.environ["AWS_REGION_NAME"] = "us-east-1"

# sometimes the model refused to generate content due to internal guardrails
# so this is a custom exception to catch that error
class NoContentGeneratedException(Exception):
    pass

# Canned code to use in case the model generates misformatted code
# so this code will also cause the test to fail so in that sense the overall
# accuracy of the model for this benchmark remain unaffected and this code simply
# helps the harness to move to the next problem in the benchmark

FAILED_RESPONSE = """
import sys

def main():
    input = sys.stdin.read
    data = input().split()
    
    # do nothing, this is a canned response so that the eval for
    # this task can silently fail

    print(data)

if __name__ == "__main__":
    main()
"""

# Regular expression pattern to extract Python code blocks from text
# Matches content between ```python and ``` markers, capturing everything in between
REGEX_FOR_PY_CODE_EXTRACTION: str = r"```python\n(.*?)```"

def _process_task(model_name: str, formatted_prompt: str, inference_params: dict) -> str:
    """
    Runs inference for a prompt using the specified model. Retry after sleep logic is in place
    in case of exceptions.
    
    Args:
        model_name (str): The Amazon Bedrock model id to use, needs to start with "bedrock/"
        formatted_prompt (str): Prompt for inference
        inference_params (Dict): inference parameters such as max_tokens, tempertature and n
    Returns:
        str: The completion generated by the model
    
    Note:
        - Raises exception in case retries are exhausedted
    """
    max_retries: int = 10 # set to a rather higher value for Amazon Nova
    retry_delay: int = 60  # seconds
    print(f"formatted_prompt={formatted_prompt}")
    for attempt in range(max_retries):
        try:
            # run inference
            response = completion(
                    model=model_name,
                    model_id=None,
                    messages=[{"role": "user", "content": formatted_prompt}],
                    max_tokens=inference_params["max_tokens"],
                    temperature=inference_params["temperature"],
                    n=inference_params["n"],
                )
            # Debug: logger.info raw response
            logger.info(f"Raw Response: {response}")
            # check if we received an empty response, for example the model saying something like
            # "The generated text has been blocked by our content filters."
            if response['usage']['completion_tokens'] == 0:
                content = response["choices"][0]["message"]["content"]
                raise NoContentGeneratedException(f"completion tokens is 0, content={content}")
            return response

        except NoContentGeneratedException as e:
            if attempt < max_retries - 1:
                # increase delay with every retry and add some random jitter to the delay
                this_retry_delay = retry_delay * (attempt + 1) + random.randint(1, 10)
                logger.error(f"{e}, task on attempt {attempt + 1}. Waiting {retry_delay} seconds...")
                time.sleep(this_retry_delay)
                continue
            else:
                logger.error(f"max retries exceeded for task")
                raise  # Re-raise the exception if we've exhausted all retries    
        except RateLimitError as e:
            if attempt < max_retries - 1:
                # increase delay with every retry and add some random jitter to the delay
                this_retry_delay = retry_delay * (attempt + 1) + random.randint(1, 10)
                logger.error(f"{e}, task on attempt {attempt + 1}. Waiting {this_retry_delay} seconds...")
                time.sleep(this_retry_delay)
                continue
            else:
                logger.error(f"max retries exceeded for task")
                raise  # Re-raise the exception if we've exhausted all retries
                
        except Exception as e:
            logger.error(f"Unexpected error processing task: {str(e)}")
            raise

def gen_code(prompt):
    """
    Use this tool only when you need to generate code based on the problem. The input is the Problem Statement. The tool returns code that the customer can use.
    """
    prompt_ending = """Please reply with a Python 3 solution to the below problem. Make sure
                        to wrap your code in '```python' and '```' Markdown delimiters, and
                        include exactly one block of code with the entire solution.
                        Just return the code, do not provide any explanation."""
    
    inference_params = dict(max_tokens=2000, temperature=0.1, n=1)
    generated_text = _process_task("bedrock/amazon.nova-micro-v1:0", prompt + prompt_ending, inference_params)
    return generated_text
